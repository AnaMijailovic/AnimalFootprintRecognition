{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_with_GPU_(2)_(1) (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Tensorflow with GPU\n",
        "\n",
        "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Editâ†’Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "Next, we'll confirm that we can connect to the GPU with tensorflow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4c868fc-2f1c-4d51-809b-ba8ac99dddda"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE3HmorBDtje",
        "colab_type": "text"
      },
      "source": [
        "# Defining constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuSnMfD1qVqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = 227\n",
        "number_of_channels = 3\n",
        "number_of_classes = 10\n",
        "training_path = \"/Data/TrainingData\"\n",
        "validation_path = \"/Data/ValidationData\"\n",
        "test_path = \"/Data/TestData\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEFXtRdhD2bO",
        "colab_type": "text"
      },
      "source": [
        "# The main part of the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nH_GmpfnHizh",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Lambda, BatchNormalization\n",
        "from keras.utils import  np_utils\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.optimizers import SGD, Adam\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "import cv2\n",
        "from keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def load_cnn_model(model_path):\n",
        "    model = create_nn_model()\n",
        "    model.load_weights(model_path)\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, train_path, test_path):\n",
        "    print(\"\\n*** PREDICTING ON TEST DATA ***\")\n",
        " \n",
        "    print(\"\\n   Loading images...\")\n",
        "    train_x, train_y = load_images(train_path)\n",
        "    test_x, test_y = load_images(test_path)\n",
        "    print(\"Test num: \" + str(len(test_x)))\n",
        "    print(\"   DONE! Images are loaded.\")\n",
        "\n",
        "    datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
        "    # calculate mean on training dataset\n",
        "    datagen.fit(train_x)\n",
        "    # prepare an iterators to scale images\n",
        "    print(\"Mean: \", datagen.mean)\n",
        "    print(\"Std: \", datagen.std)\n",
        "    test_iterator = datagen.flow(test_x, test_y, batch_size=64)\n",
        "    print(\"Mean: \", datagen.mean)\n",
        "    print(\"Std: \", datagen.std)\n",
        "\n",
        "  \n",
        "    predictions = model.predict_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
        "\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # print classification report\n",
        "    # print(classification_report(test_iterator.classes, predictions,\n",
        "\t  #              target_names=test_iterator.class_indices.keys()))\n",
        "\n",
        "    test_y = np.argmax(test_y, axis=1)\n",
        "\n",
        "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
        "    print('Test Accuracy: %.3f' % (acc * 100))\n",
        "    print(test_y)\n",
        "    print(predictions)\n",
        "    print(accuracy_score(test_y, predictions))\n",
        "\n",
        "\n",
        "def fit_model(epochs, batch_size):\n",
        "\n",
        "    # load images\n",
        "    print(\"\\n\\tLoading training images...\")\n",
        "    train_x, train_y = load_images(training_path)\n",
        "\n",
        "    print(\"\\n\\tLoading validation images...\")\n",
        "    x_validation, y_validation = load_images(validation_path)\n",
        "    \n",
        "    # create generator to center images\n",
        "    datagen = ImageDataGenerator(rescale=1 / 255.0)\n",
        "    # calculate mean on training dataset\n",
        "    datagen.fit(train_x)\n",
        "    # prepare an iterators to scale images\n",
        "    train_iterator = datagen.flow(train_x, train_y, batch_size=64)\n",
        "    test_iterator = datagen.flow(x_validation, y_validation, batch_size=batch_size)\n",
        "\n",
        "    print('Mean: ', datagen.mean)\n",
        "    mean = datagen.mean\n",
        "    print('Std: ', datagen.std)\n",
        "    std = datagen.std\n",
        "    print(\"Creating nn model ...\")\n",
        "    model = create_nn_model()\n",
        "    print(\"Model is created\")\n",
        "\n",
        "    history = model.fit_generator(train_iterator, validation_data=test_iterator, steps_per_epoch=len(train_iterator), epochs=epochs)\n",
        "    # evaluate model \n",
        "    _, acc = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
        "    print('Test Accuracy: %.3f' % (acc * 100))\n",
        "\n",
        "    # save model weights and return history\n",
        "    model.save_weights('network.h5')\n",
        "    return history\n",
        "\n",
        "\n",
        "def load_images(folder):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for wbcType in listdir(folder):\n",
        "        for imgName in listdir(folder + '/' + wbcType):\n",
        "            image = cv2.imread(folder + '/' + wbcType + '/' + imgName)\n",
        "\n",
        "            if image is not None:\n",
        "                dim = (image_size, image_size)\n",
        "                # konvertovanje iz BGR u RGB model boja (OpenCV ucita sliku kao BGR)\n",
        "                img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                # konvertovanje u grayscale sliku\n",
        "                # img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY) \n",
        "\n",
        "                resized = cv2.resize(img_rgb, dim, interpolation=cv2.INTER_AREA)\n",
        "                res =  np.reshape(resized, (image_size, image_size, 3))\n",
        "\n",
        "                image_array = np.asarray(res)\n",
        "                x.append(image_array)\n",
        "                y.append(wbcType)\n",
        "\n",
        "    y_array = np.asarray(y)\n",
        "\n",
        "    # encode y data\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(y_array)\n",
        "    encoded_y = encoder.transform(y_array)\n",
        "    y = np_utils.to_categorical(encoded_y)\n",
        "\n",
        "     \n",
        "    return np.asarray(x), y\n",
        "\n",
        "\n",
        "def create_nn_model():\n",
        "    # Instantiate an empty model\n",
        "    model = Sequential()\n",
        "\n",
        "    # 1st Convolutional Layer\n",
        "    model.add(Conv2D(filters=96, input_shape=(image_size, image_size, number_of_channels), kernel_size=(11, 11),\n",
        "                     strides=(4, 4), padding=\"valid\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Max Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
        "\n",
        "    # 2nd Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(11, 11), strides=(1, 1), padding=\"valid\",\n",
        "                       kernel_regularizer=regularizers.l2(0.01),\n",
        "                     bias_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Max Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
        "\n",
        "    # 3rd Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
        "                     \n",
        "                     kernel_regularizer=regularizers.l2(0.01),\n",
        "                     bias_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # 4th Convolutional Layer\n",
        "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
        "                     kernel_regularizer=regularizers.l2(0.01),\n",
        "                     bias_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # 5th Convolutional Layer\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
        "                     kernel_regularizer=regularizers.l2(0.01),\n",
        "                     bias_regularizer=regularizers.l2(0.01)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Max Pooling\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
        "\n",
        "    # Passing it to a Fully Connected layer\n",
        "    model.add(Flatten())\n",
        "    # 1st Fully Connected Layer\n",
        "    model.add(Dense(4096, input_shape=(image_size, image_size, number_of_channels)))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Add Dropout to prevent overfitting\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # 2nd Fully Connected Layer\n",
        "    model.add(Dense(4096))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # 3rd Fully Connected Layer\n",
        "    model.add(Dense(1000))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    # Add Dropout\n",
        "    model.add(Dropout(0.4))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(number_of_classes)) \n",
        "    model.add(Activation(\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Compile the model\n",
        "    # sgd = SGD(lr=0.005, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # sgd = SGD(lr=0.1, decay=1e-6, momentum=0.0, nesterov=False)\n",
        "    # print(\"sgd\")\n",
        "    # opt = SGD(lr=0.01)\n",
        "    learning_rate = 0.001\n",
        "    decay_rate = learning_rate / 100\n",
        "    sgd = SGD(lr= 0.005, momentum=0.8, decay= decay_rate, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=sgd,\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def create_vgg_model():\n",
        "\n",
        "  model = Sequential()\n",
        "  # model.add(Lambda(lambda x: x / 127.5 - 1., input_shape=(image_size, image_size, number_of_channels,), output_shape=(image_size, image_size, number_of_channels,)))\n",
        "  # First Convolotional Layer\n",
        "  model.add(Conv2D(input_shape=(image_size, image_size, number_of_channels),filters=64,kernel_size=(3,3),padding=\"same\", \n",
        "                   activation=\"relu\"))\n",
        "  # Second Convolutional Layer\n",
        "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  # model.add(BatchNormalization())\n",
        "\n",
        "  # Third Convolutional Layer\n",
        "  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  # model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  # model.add(BatchNormalization())\n",
        "\n",
        "  # Fourth Convolutional Layer\n",
        "  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  # model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  # model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  # model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))\n",
        "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  # model.add(BatchNormalization())\n",
        "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
        "                   kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "  # model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=4096,activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(Dense(units=4096,activation=\"relu\"))\n",
        "  # Add Dropout\n",
        "  model.add(Dropout(0.4))\n",
        "  # Output Layer\n",
        "  model.add(Dense(number_of_classes)) # Da li je ovo broj klasa?\n",
        "  model.add(Activation(\"softmax\"))\n",
        "\n",
        "  # model.summary()\n",
        "\n",
        "  # opt = Adam(lr=0.01)\n",
        "  learning_rate = 0.01\n",
        "  decay_rate = learning_rate / 100\n",
        "  opt = SGD(lr= 0.007, momentum=0.8, decay= decay_rate, nesterov=True)\n",
        "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_acc_diagram(history):\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.savefig('accuracy.pdf')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_loss_diagram(history):\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.savefig('loss.pdf')\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcObt0qD-s9",
        "colab_type": "text"
      },
      "source": [
        "# Main function with menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtXCjFWnq6Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "\n",
        "def menu():\n",
        "\n",
        "    proceed = True\n",
        "    while proceed:\n",
        "\n",
        "       # print(\"\\n1. Data Augmentation \")\n",
        "        print(\"1. Train model\")\n",
        "        print(\"2. Predict\")\n",
        "        print(\"0. Exit\")\n",
        "\n",
        "        select = int(input(\"Please select an option: \\n\"))\n",
        "\n",
        "        if select == 1:\n",
        "            print(\"Calling model training ... \")\n",
        "            with tf.device('/device:GPU:0'):\n",
        "                history = fit_model(50, 128)\n",
        "                print(history.history.keys())\n",
        "                create_acc_diagram(history)\n",
        "                create_loss_diagram(history)\n",
        "        elif select == 2:\n",
        "            model = load_cnn_model(\"network.h5\")\n",
        "            predict(model, training_path, test_path)\n",
        "        elif select == 0:\n",
        "            proceed = False\n",
        "        else:\n",
        "            print(\"\\nInvalid option\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\" *** Animal Footprint Recognition started ***\\n\")\n",
        "    menu()\n",
        "    print(\"Finished\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEanzrLCEKsW",
        "colab_type": "text"
      },
      "source": [
        "# Unzip uploaded data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-ZV8TwJYKOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/Data.zip','r') as zip_ref:\n",
        "     zip_ref.extractall(\"/\")"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}