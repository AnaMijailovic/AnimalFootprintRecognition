{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMce8muBqXQP"
   },
   "source": [
    "# Tensorflow with GPU\n",
    "\n",
    "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oM_8ELnJq_wd"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sXnDmXR7RDr2"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YE3HmorBDtje"
   },
   "source": [
    "# Defining constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TuSnMfD1qVqg"
   },
   "outputs": [],
   "source": [
    "image_size = 227\n",
    "number_of_channels = 1\n",
    "number_of_classes = 10\n",
    "training_path = \"/Data/TrainingData\"\n",
    "validation_path = \"/Data/ValidationData\"\n",
    "test_path = \"/Data/TestData\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OEFXtRdhD2bO"
   },
   "source": [
    "# The main part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH_GmpfnHizh"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Lambda, BatchNormalization\n",
    "from keras.utils import  np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import SGD, Adam\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import cv2\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def load_cnn_model(model_file_path):\n",
    "    \"\"\"\n",
    "      Loads saved model\n",
    "    \"\"\"\n",
    "    cnn_model = create_nn_model()\n",
    "    cnn_model.load_weights(model_file_path)\n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "def predict(cnn_model, train_path, test_path):\n",
    "    print(\"\\n*** PREDICTING ON TEST DATA ***\")\n",
    " \n",
    "    print(\"\\n   Loading images...\")\n",
    "    train_x, train_y = load_images(train_path)\n",
    "    test_x, test_y = load_images(test_path)\n",
    "    print(\"Test num: \" + str(len(test_x)))\n",
    "    print(\"   DONE! Images are loaded.\")\n",
    "\n",
    "    # Normalize and center images with ImageDataGenerator\n",
    "    image_datagen = ImageDataGenerator(rescale=1 / 255.0, featurewise_center=True)\n",
    "    # calculate statistics on training dataset\n",
    "    image_datagen.fit(train_x)\n",
    "\n",
    "    # iterators to scale images\n",
    "    test_iterator = image_datagen.flow(test_x, test_y, batch_size=64, shuffle=True)\n",
    "\n",
    "    predictions = cnn_model.predict_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    test_y = np.argmax(test_y, axis=1)\n",
    "    _, acc = cnn_model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "    print('Test Accuracy: %.3f' % (acc * 100))\n",
    "\n",
    "\n",
    "def fit_model(epochs, batch_size):\n",
    "\n",
    "    # load images\n",
    "    print(\"\\n\\tLoading training images...\")\n",
    "    train_x, train_y = load_images(training_path)\n",
    "\n",
    "    print(\"\\n\\tLoading validation images...\")\n",
    "    x_validation, y_validation = load_images(validation_path)\n",
    "    \n",
    "    # create generator to center images\n",
    "    image_datagen = ImageDataGenerator(rescale=1 / 255.0, featurewise_center=True)\n",
    "    # calculate statistics on training dataset\n",
    "    image_datagen.fit(train_x)\n",
    "\n",
    "    # iterators to scale images\n",
    "    train_iterator = image_datagen.flow(train_x, train_y, shuffle=True, batch_size=64)\n",
    "    validation_iterator = image_datagen.flow(x_validation, y_validation, batch_size=batch_size)\n",
    "\n",
    "    print(\"Creating cnn model ...\")\n",
    "    model = create_nn_model()\n",
    "    print(\"Model is created\")\n",
    "    \n",
    "    # Save the best model\n",
    "    checkpoint = ModelCheckpoint('model.h5', verbose=1,\n",
    "                                 monitor='val_loss',save_best_only=True, mode='auto')\n",
    "    history = model.fit_generator(train_iterator, validation_data=validation_iterator,callbacks=[checkpoint],\n",
    "                                  steps_per_epoch=len(train_iterator), epochs=epochs, shuffle=True)\n",
    "    # evaluate model \n",
    "    _, acc = model.evaluate_generator(validation_iterator, steps=len(validation_iterator), verbose=0)\n",
    "    print('Validation Accuracy: %.3f' % (acc * 100))\n",
    "\n",
    "    # save model weights and return history data\n",
    "    model.save_weights('network.h5')\n",
    "    return history\n",
    "\n",
    "\n",
    "def load_images(folder_path):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    # folder names are animal class names\n",
    "    for class_name in listdir(folder_path):\n",
    "        for image_name in listdir(folder_path + '/' + class_name):\n",
    "            image = cv2.imread(folder_path + '/' + class_name + '/' + image_name)\n",
    "\n",
    "            if image is not None:\n",
    "                dimensions = (image_size, image_size)\n",
    "                # convert from BGR to RGB (OpenCV loads image as BGR)\n",
    "                img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # convert to grayscale\n",
    "                img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY) \n",
    "\n",
    "                # resize\n",
    "                resized = cv2.resize(img_gray, dimensions, interpolation=cv2.INTER_AREA)\n",
    "                res =  np.reshape(resized, (image_size, image_size, number_of_channels))\n",
    "\n",
    "                image_array = np.asarray(res)\n",
    "                x.append(image_array)\n",
    "                y.append(class_name)\n",
    "\n",
    "    y_array = np.asarray(y)\n",
    "\n",
    "    # encode y data\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_array)\n",
    "    encoded_y = encoder.transform(y_array)\n",
    "    y = np_utils.to_categorical(encoded_y)\n",
    "\n",
    "     \n",
    "    return np.asarray(x), y\n",
    "\n",
    "\n",
    "def create_nn_model():\n",
    "  # Instantiate an empty model\n",
    "    model = Sequential()\n",
    "\n",
    "    # First Convolutional Layer\n",
    "    model.add(Conv2D(filters=96, input_shape=(image_size, image_size, number_of_channels), kernel_size=(11, 11),\n",
    "                     strides=(4, 4), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
    "    # model.add(BatchNormalization())\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(11, 11), strides=(1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Fourth Convolutional Layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Fifth Convolutional Layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"valid\",\n",
    "                     kernel_regularizer=regularizers.l2(0.01),\n",
    "                     bias_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Max Pooling\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\"))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # Fully Connected layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, input_shape=(image_size, image_size, number_of_channels)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    # Add Dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # 2nd Fully Connected Layer\n",
    "    # model.add(Dense(4096))\n",
    "    # model.add(Activation(\"relu\"))\n",
    "    # Add Dropout\n",
    "    # model.add(Dropout(0.4))\n",
    "\n",
    "    # 3rd Fully Connected Layer\n",
    "    # model.add(Dense(1000))\n",
    "    # model.add(Activation(\"relu\"))\n",
    "    # Add Dropout\n",
    "    # model.add(Dropout(0.4))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(number_of_classes)) \n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = learning_rate / 100\n",
    "    sgd = SGD(lr= learning_rate, momentum=0.8, decay= decay_rate, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# not used\n",
    "def create_vgg_model():\n",
    "\n",
    "  model = Sequential()\n",
    " \n",
    "  # First Convolotional Layer\n",
    "  model.add(Conv2D(input_shape=(image_size, image_size, number_of_channels),\n",
    "                   filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "\n",
    "  # Second Convolutional Layer\n",
    "  model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "  # model.add(BatchNormalization())\n",
    "\n",
    "  # Third Convolutional Layer\n",
    "  model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  # model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "  # model.add(BatchNormalization())\n",
    "\n",
    "  # Fourth Convolutional Layer\n",
    "  model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  # model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  # model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "  # model.add(BatchNormalization())\n",
    "\n",
    "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                   kernel_regularizer=regularizers.l2(0.01)))\n",
    "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "  # model.add(BatchNormalization())\n",
    "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  # model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "  model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\",\n",
    "                   kernel_regularizer=regularizers.l2(0.01)))\n",
    "  model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "  # model.add(BatchNormalization())\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(units=4096,activation=\"relu\", kernel_regularizer=regularizers.l2(0.01)))\n",
    "  model.add(Dense(units=4096,activation=\"relu\"))\n",
    "  # Add Dropout to prevent overfitting\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Dense(number_of_classes))\n",
    "  model.add(Activation(\"softmax\"))\n",
    "\n",
    "  learning_rate = 0.01\n",
    "  decay_rate = learning_rate / 100\n",
    "  opt = SGD(lr= 0.007, momentum=0.8, decay= decay_rate, nesterov=True)\n",
    "  model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "def create_acc_diagram(history):\n",
    "    \"\"\"\n",
    "      Creates training/validation accuracy diagram\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig('accuracy.pdf')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_loss_diagram(history):\n",
    "    \"\"\"\n",
    "      Creates training/validation loss function diagram\n",
    "    \"\"\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('loss.pdf')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qjcObt0qD-s9"
   },
   "source": [
    "# Main function with menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GtXCjFWnq6Vm"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "def menu():\n",
    "\n",
    "    proceed = True\n",
    "    while proceed:\n",
    "\n",
    "       # print(\"\\n1. Data Augmentation \")\n",
    "        print(\"1. Train model\")\n",
    "        print(\"2. Predict\")\n",
    "        print(\"0. Exit\")\n",
    "\n",
    "        select = int(input(\"Please select an option: \\n\"))\n",
    "\n",
    "        if select == 1:\n",
    "            print(\"Calling model training ... \")\n",
    "            with tf.device('/device:GPU:0'):\n",
    "                history = fit_model(100, 64)\n",
    "                print(history.history.keys())\n",
    "                create_acc_diagram(history)\n",
    "                create_loss_diagram(history)\n",
    "        elif select == 2:\n",
    "            model = load_cnn_model(\"model.h5\")\n",
    "            predict(model, training_path, test_path)\n",
    "        elif select == 0:\n",
    "            proceed = False\n",
    "        else:\n",
    "            print(\"\\nInvalid option\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\" *** Animal Footprint Recognition started ***\\n\")\n",
    "    menu()\n",
    "    print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEanzrLCEKsW"
   },
   "source": [
    "# Unzip uploaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-ZV8TwJYKOc"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('/content/Data.zip','r') as zip_ref:\n",
    "     zip_ref.extractall(\"/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_with_GPU_(2)_(1)_(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
